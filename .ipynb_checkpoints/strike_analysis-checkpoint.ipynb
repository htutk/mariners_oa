{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020 _'isStrike'_ Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective in this project to determine whethere a pitch is called a ball or a strike as accurately as possible based on the given data. The [test set in the data folder](./data/2020-test.csv) includes the pitches we have to estimate. The [trianing set](./data/2020-train.csv) on the other hand has the data we can use to train and analyze to result in the most accurate algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can determine that the classification type of machine learning (ML) alogrithm is the best for this problem since the main task is to choose a category (a strike or a ball). We will start off with a common classification ML algorithm: Logistic Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin to writing codes, we have to analyze the data we are dealing with. First, we will look around the data files and see if anything is misplaced/skewed. To do so, we will use **python** and its popular libraries: _numpy_, _pandas_, and _matplotlib_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the datasets\n",
    "train_df = pd.read_csv('./data/2020-train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the train_df, particularly the column we are interested in (`pitch_type`), we can see that some of the pitches are ambiguous. For example, a pitch like `StrikeSwinging`, there is no way to tell whether the pitcher thew a strike or a ball based on the data and the way it was recorded. Therefore, for our purposes, we will only include the rows where the batter did not swing and either _ball_ or _strike_ was recorded. To filter the rows, we write a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df):\n",
    "    strikes = np.array(df['pitch_call'] == 'StrikeCalled')\n",
    "    balls = np.array(df['pitch_call'] == 'BallCalled')\n",
    "    filtered = strikes | balls\n",
    "    return df[filtered]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to notice is that some of Right/Left data are recorded as R/L. In this case, we can ignore these rows or try to fix the issue. Since it is relatively easy, we decided to fix it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some data are skewed\n",
    "# for ex, pitcher_side has some cells with R, L \n",
    "# instead of right, left\n",
    "def fix_skewed_data(df):\n",
    "    df['pitcher_side'] = np.where(df.pitcher_side == 'R', 'Right', df.pitcher_side)\n",
    "    df['pitcher_side'] = np.where(df.pitcher_side == 'L', 'Left', df.pitcher_side)\n",
    "    df['batter_side'] = np.where(df.batter_side == 'R', 'Right', df.batter_side)\n",
    "    df['batter_side'] = np.where(df.batter_side == 'L', 'Left', df.batter_side)\n",
    "    return df\n",
    "\n",
    "# Prepare training dataset\n",
    "train_df = filter_df(train_df).copy()\n",
    "train_df = fix_skewed_data(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then look at the columns and decide the irrelevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pitcher_id', 'pitcher_side', 'batter_id', 'batter_side', 'stadium_id',\n",
       "       'umpire_id', 'catcher_id', 'inning', 'top_bottom', 'outs', 'balls',\n",
       "       'strikes', 'release_speed', 'vert_release_angle', 'horz_release_angle',\n",
       "       'spin_rate', 'spin_axis', 'tilt', 'rel_height', 'rel_side', 'extension',\n",
       "       'vert_break', 'induced_vert_break', 'horz_break', 'plate_height',\n",
       "       'plate_side', 'zone_speed', 'vert_approach_angle',\n",
       "       'horz_approach_angle', 'zone_time', 'x55', 'y55', 'z55', 'pitch_type',\n",
       "       'pitch_call', 'pitch_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can determine that some of the columns are not correlated to DV at all. For example, most of the ids are irrelevant to the call of the pitch (however, note that particular umpires may have some tendacies to call the pitches more in 'their' ways). Thus, we will exclude them from the list of independent variables (IVs). To do so, we will simply drop the particular colums from the df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.copy()\n",
    "X = X.drop(['pitcher_id', 'batter_id', 'stadium_id', \\\n",
    "                        'umpire_id', 'catcher_id', 'pitch_call', \\\n",
    "                        'pitch_id', 'tilt'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to exclude the 'tilt' column because it is a categorical variable with many different values; the column will make the df much bigger and impossible to control. For future improvements, we can find a way to give scores to different tilts and that can be easily included in IVs list. Since we are done with IV matrix, we can then set up the DV column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df[['pitch_call']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that upon this point, we have not included anything from [test.csv](./data/2020-test.csv). The reason is we will first determine which model (logistic regression, knn, kernel svm, naives bayes, decision tree, or random forest) has the best accuracy among 80-20 train/test split in train data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the columns are categorical. Thus, we must convert them to numbers using dummy variables method to fit/transfomr into ML algorithm. We do this by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical variables\n",
    "# Create dummy variables for pitcher_side, batter_side, and pitch_type\n",
    "# note all the dummies should be 1 category less than their original # of categories\n",
    "dummies_pitcher_side = pd.get_dummies(X_train[['pitcher_side']])\n",
    "dummies_pitcher_side = dummies_pitcher_side.iloc[:, 1:]\n",
    "\n",
    "dummies_batter_side = pd.get_dummies(X_train[['batter_side']])\n",
    "dummies_batter_side = dummies_batter_side.iloc[:, 1:]\n",
    "\n",
    "dummies_pitch_type = pd.get_dummies(X_train[['pitch_type']])\n",
    "dummies_pitch_type = dummies_pitch_type.iloc[:, 0:5]\n",
    "\n",
    "X_train = X_train.drop(['pitcher_side', 'batter_side', \\\n",
    "                        'pitch_type'], axis = 1)\n",
    "dummies = pd.concat([dummies_pitcher_side, dummies_batter_side, dummies_pitch_type],\n",
    "                    axis = 1)\n",
    "X_train = pd.concat([dummies, X_train], axis = 1)\n",
    "\n",
    "# convert strikes and balls to zeros/ones\n",
    "dummies_strikes = pd.get_dummies(y_train[['pitch_call']])\n",
    "dummies_strikes = dummies_strikes.iloc[:, 1:]\n",
    "y_train = np.array([x for x in dummies_strikes['pitch_call_StrikeCalled']])\n",
    "\n",
    "\n",
    "# convert categorical variables for test values\n",
    "# Create dummy variables for pitcher_side, batter_side, and pitch_type\n",
    "# note all the dummies should be 1 category less than their original # of categories\n",
    "dummies_pitcher_side = pd.get_dummies(X_test[['pitcher_side']])\n",
    "dummies_pitcher_side = dummies_pitcher_side.iloc[:, 1:]\n",
    "\n",
    "dummies_batter_side = pd.get_dummies(X_test[['batter_side']])\n",
    "dummies_batter_side = dummies_batter_side.iloc[:, 1:]\n",
    "\n",
    "dummies_pitch_type = pd.get_dummies(X_test[['pitch_type']])\n",
    "dummies_pitch_type = dummies_pitch_type.iloc[:, 0:5]\n",
    "\n",
    "X_test = X_test.drop(['pitcher_side', 'batter_side', \\\n",
    "                        'pitch_type'], axis = 1)\n",
    "dummies = pd.concat([dummies_pitcher_side, dummies_batter_side, dummies_pitch_type],\n",
    "                    axis = 1)\n",
    "X_test = pd.concat([dummies, X_test], axis = 1)\n",
    "\n",
    "# convert strikes and balls to zeros/ones\n",
    "dummies_strikes = pd.get_dummies(y_test[['pitch_call']])\n",
    "dummies_strikes = dummies_strikes.iloc[:, 1:]\n",
    "y_test = np.array([x for x in dummies_strikes['pitch_call_StrikeCalled']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pd.get_dummies()` will give two columns if there is two original categories. For example, in y_train, since there is _StrikeCalled_ and _BallCalled_, there will be two columns returned with the same information. In this line `dummies_strikes = pd.get_dummies(y_train[['pitch_call']])`, we will receive a df with two columns: `BallCalled` and `StrikeCalled`, filled with 0's and 1's. One column is the opposite of the other, and we only need one.\n",
    "\n",
    "Since we want _strike_ to be **1**, we choose `StrikeCalled`.\n",
    "\n",
    "We can then do the same to test set, except for y_test since we will be predicting later on. The code can be found [here](\"./logistic_regression.py\"). We are then ready to perform feature-scaling as the final step before training the dataset. We can use the standard scaler included in _scikit-learn_ library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\alexa\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\alexa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These warnings are just notions of ints and doubles being converted to floats all together. They should not impact the results.\n",
    "\n",
    "Some values in `X_train` and `X_test` are NaN and cannot be fitted into the model. Thus, these rows are removed accordingly (from both `X` and `y` matrices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(X_train)\n",
    "n = len(X_train[0])\n",
    "rows_to_delete = []\n",
    "\n",
    "for x in range(0, m):\n",
    "    nan_found = False\n",
    "    for y in range(0, n):\n",
    "        if np.isnan(X_train[x][y]):\n",
    "            nan_found = True\n",
    "            break\n",
    "    if nan_found:\n",
    "        rows_to_delete.append(x)        \n",
    "\n",
    "X_train = np.delete(X_train, rows_to_delete, axis = 0)\n",
    "y_train = np.delete(y_train, rows_to_delete, axis = 0)\n",
    "\n",
    "m = len(X_test)\n",
    "n = len(X_test[0])\n",
    "rows_to_delete = []\n",
    "\n",
    "for x in range(0, m):\n",
    "    nan_found = False\n",
    "    for y in range(0, n):\n",
    "        if np.isnan(X_test[x][y]):\n",
    "            nan_found = True\n",
    "            break\n",
    "    if nan_found:\n",
    "        rows_to_delete.append(x)\n",
    "\n",
    "X_test = np.delete(X_test, rows_to_delete, axis = 0)\n",
    "y_test = np.delete(y_test, rows_to_delete, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to test out first model: Logistic Regression. The `scikit-learn` library has very useful models, so we will go ahead and use them simply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the models may take a few minutes to complete modeling. The confusion matrix result from Log Regression is as follow:\n",
    "\n",
    "|Actual/Pred | 0      | 1     |\n",
    "|---------------|-------|-------|\n",
    "| 0 | 37440 | 3706 |\n",
    "| 1 | 16131 | 3351 |\n",
    "\n",
    "The accuracy from the confusion matrix is measured as below:\n",
    "$$ Accuracy = \\frac{(TP + TN)}{(TP + TN + FP + FN)} $$\n",
    "\n",
    "$$ Logistion Regression Accuracy = \\frac{37440 + 3351}{37440 + 3351 + 16131 + 3351} = 0.6728 = 68.27\\%  $$\n",
    "\n",
    "We can then calculate the accuracy profiles from other models. The code is given [here](./logistic_regression_multiple_tests.py).\n",
    "\n",
    "\n",
    "##### KNN\n",
    "|Actual/Pred | 0      | 1     |\n",
    "|---------------|-------|-------|\n",
    "| 0 | 35561 | 5585 |\n",
    "| 1 | 4925 | 14557 |\n",
    "\n",
    "$$ KNN Accuracy = 82.66\\%$$\n",
    "\n",
    "\n",
    "##### Kervel SVM\n",
    "|Actual/Pred | 0      | 1     |\n",
    "|---------------|-------|-------|\n",
    "| 0 | 38915 | 2231 |\n",
    "| 1 | 2350 | 17132 |\n",
    "\n",
    "$$ Kernel SVM Accuracy = 92.44\\%$$\n",
    "\n",
    "\n",
    "##### Naive Bayes\n",
    "|Actual/Pred | 0      | 1     |\n",
    "|---------------|-------|-------|\n",
    "| 0 | 35629 | 5517 |\n",
    "| 1 | 5216 | 14266 |\n",
    "\n",
    "$$ Naive Bayes Accuracy = 82.3\\%$$\n",
    "\n",
    "\n",
    "##### Decision Tree\n",
    "|Actual/Pred | 0      | 1     |\n",
    "|---------------|-------|-------|\n",
    "| 0 | 38192 | 2954 |\n",
    "| 1 | 3115 | 16367 |\n",
    "\n",
    "$$ Decision Tree Accuracy = 89.99\\%$$\n",
    "\n",
    "\n",
    "##### Random Forest\n",
    "|Actual/Pred | 0      | 1     |\n",
    "|---------------|-------|-------|\n",
    "| 0 | 35561 | 5585 |\n",
    "| 1 | 4925 | 14557 |\n",
    "\n",
    "$$ Random Forest Accuracy = 91.86\\%$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above, **Kernel SVM** model is the most accurate algorithm with over 92% accuracy. However, it is good to note that _Kernel SVM_ model took more than 10 minutes to complete, whereas _Random Forest_ took only a few seconds and produced a very strong result. Since speed could be a potential factor, I decided to choose **Random Forest** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest --> 91.86%\n",
    "# Fitting Random Forest Classification to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are then ready to predict the pitches in test.csv. First, we have to fix the data like we did for training dataset. We change R/L to Right/Left, convert categorical variables, and feature-scale the data. After feature-scaling, about _2000 rows_ or 1.5 percent of the data removed. We will take note of it and append it with either a strike or a ball at the end (50% chance of getting it right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test.csv data\n",
    "# Prepare the df\n",
    "test_df = pd.read_csv('./data/2020-test.csv')\n",
    "test_df = fix_skewed_data(test_df)\n",
    "X_test = test_df.copy()\n",
    "X_test = X_test.drop(['pitcher_id', 'batter_id', 'stadium_id', \\\n",
    "                        'umpire_id', 'catcher_id', 'is_strike', \\\n",
    "                        'pitch_id', 'tilt'], axis = 1)\n",
    "\n",
    "# convert categorical variables\n",
    "# Create dummy variables for pitcher_side, batter_side, and pitch_type\n",
    "# note all the dummies should be 1 category less than their original # of categories\n",
    "dummies_pitcher_side = pd.get_dummies(X_test[['pitcher_side']])\n",
    "dummies_pitcher_side = dummies_pitcher_side.iloc[:, 1:]\n",
    "\n",
    "dummies_batter_side = pd.get_dummies(X_test[['batter_side']])\n",
    "dummies_batter_side = dummies_batter_side.iloc[:, 1:]\n",
    "\n",
    "dummies_pitch_type = pd.get_dummies(X_test[['pitch_type']])\n",
    "dummies_pitch_type = dummies_pitch_type.iloc[:, 0:5]\n",
    "\n",
    "X_test = X_test.drop(['pitcher_side', 'batter_side', \\\n",
    "                        'pitch_type'], axis = 1)\n",
    "dummies = pd.concat([dummies_pitcher_side, dummies_batter_side, dummies_pitch_type],\n",
    "                    axis = 1)\n",
    "X_test = pd.concat([dummies, X_test], axis = 1)\n",
    "\n",
    "# feature scale\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "# remove NaN values\n",
    "# 2263 rows removed\n",
    "m = len(X_test)\n",
    "n = len(X_test[0])\n",
    "rows_to_delete = []\n",
    "\n",
    "for x in range(0, m):\n",
    "    nan_found = False\n",
    "    for y in range(0, n):\n",
    "        if np.isnan(X_test[x][y]):\n",
    "            nan_found = True\n",
    "            break\n",
    "    if nan_found:\n",
    "        rows_to_delete.append(x)\n",
    "        \n",
    "X_test = np.delete(X_test, rows_to_delete, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the results\n",
    "y_test = classifier.predict(X_test)\n",
    "# append with balls if a data in a row is NaN\n",
    "result = []\n",
    "index = 0\n",
    "y_index = 0\n",
    "\n",
    "while y_index < len(y_test):\n",
    "    if index in rows_to_delete:\n",
    "        result.append(0)\n",
    "    else:\n",
    "        result.append(y_test[y_index])\n",
    "        y_index += 1\n",
    "    index += 1\n",
    "\n",
    "result = pd.DataFrame(result)\n",
    "result.columns = ['Strikes']\n",
    "pitch_id = test_df[['pitch_id']]\n",
    "result = pd.concat([result, pitch_id], axis = 1)\n",
    "result.to_csv('./result/2020-test-predictions.csv',\n",
    "              index = None,\n",
    "              encoding = 'utf-8',\n",
    "              header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are saved in this [file](./result/2020-test-predictions.csv). The accuracy is expected to be as follow: <br/>\n",
    "91.86% of 98.5% of all rows --> 0.9186 * 0.985 = 0.9048 <br/>\n",
    "50% of 1.5% rows --> 0.5 * 0.015 = 0.0075 <br/>\n",
    "Total = 0.9123 = 91.23%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Improvement of the model\n",
    "As mentioned above, we can **Kernel SVM** model to slightly improve the accuracies. We can also include the then-decided unrelated columns such as _tilt_ by making it _scores_ and independent variable. \n",
    "\n",
    "Another way would be to group by `umpire_id` and determine the `is_strike` since strike-calling is heavily dependent on umpires. Other general machine learnings rule apply here: more training data (preferably 80-20 split for train and test results), strike/ball recordings on all pitches types (including `foulBalls`, `strikeSwinging`), and so on.\n",
    "\n",
    "With the data we have, the accuracy result of over 90 percent seems to be decent enough for the time being."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
